{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp en Dual Encoder LSTM",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2B2P-6FZYEE",
        "colab_type": "text"
      },
      "source": [
        "드라이브 마운트 확인\n",
        "\n",
        "소스코드 참조\n",
        "\n",
        "https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
        "\n",
        "https://blog.naver.com/PostView.nhn?blogId=hist0134&logNo=220944328300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12eMUqc7NkJZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "6ead4ab2-21d7-4e33-f7ae-fe0a2eb3f2ba"
      },
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "path = \"/content/drive/My Drive/2020 1학기/자연어처리/friends\"\n",
        "print(os.listdir(path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['en_data.csv', 'en_sample.csv', 'friends_train.json', 'friends_dev.json', 'friends_test.json', '.ipynb_checkpoints', 'model.h5', 'model_by_label.h5', 'en_bert_results.csv', 'en_bert_results_round.csv', 'en_results.csv', 'en_results_round.csv', 'en_results_6.csv', 'en_results_round_6.csv', 'en_results_9.csv', 'en_results_round_9.csv', 'en_results_round_3.csv', 'en_results_3.csv', 'lstm_2_model.h5', 'lstm_2_model_25.h5', 'nlp_en_Dual_Encoder_LSTM_1.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCfDQioCZVIs",
        "colab_type": "text"
      },
      "source": [
        "임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG3rlFBwOEyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import *\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t5sPUg25B9s",
        "colab_type": "text"
      },
      "source": [
        "전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hsmlryaOHfQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "7312eaea-bd2c-4004-fe8f-61bd796c4e8f"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# text 를 전처리 해서 쓰려고 만들었는데 막상 적용해보니 성능이 떨어져서 비활성화함\n",
        "def clean_text(text):\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
        "    return text\n",
        "\n",
        "# 대략적인 전처리\n",
        "def preprocess_dataframe(df):\n",
        "  df = df.dropna() # none 제거해줌\n",
        "\n",
        "  for i in df.index:\n",
        "    i_utterance = df.at[i, 'i_utterance']\n",
        "    i_dialog = df.at[i, 'i_dialog']\n",
        "    utterance = df.at[i, 'utterance']\n",
        "\n",
        "    if i_utterance == 0: # 새로운 dialog 시작되면\n",
        "      context = \"\" #초기화\n",
        "    else:\n",
        "      context = context + \" \" + utterance\n",
        "    \n",
        "    # 한 dialog 의 텍스트를 다 붙여놓은 형태. 이전 대화의 context도 포함하기 위함\n",
        "    df.at[i, 'context'] = context\n",
        "    \n",
        "  # 길이 \n",
        "  df['l'] = df['context'].apply(lambda x: len(str(x).split(' ')))\n",
        "  print(\"mean length of sentence: \" + str(df.l.mean()))\n",
        "  print(\"max length of sentence: \" + str(df.l.max()))\n",
        "  print(\"std dev length of sentence: \" + str(df.l.std()))\n",
        "  return df\n",
        "\n",
        "# json 파일을 읽어서, kaggle sample csv를 읽은 것처럼 포맷을 정리해준다\n",
        "def read_json_make_dataframe(name='train'):  \n",
        "  file = open('%s/friends_%s.json' % (path, name), encoding='latin1')\n",
        "  d = json.load(file)\n",
        "  data = []\n",
        "  columns = ['i_dialog', 'i_utterance', 'speaker', 'utterance', 'annotation', 'emotion']\n",
        "  i_dialog = -1\n",
        "  for dialog in d:\n",
        "    i_dialog += 1 # 0부터 시작\n",
        "    i_utterance = -1\n",
        "    for row in dialog:\n",
        "      i_utterance += 1 # 0부터 시작\n",
        "      data.append([\n",
        "                   i_dialog, \n",
        "                   i_utterance, \n",
        "                   row['speaker'], \n",
        "                   row['utterance'], \n",
        "                   str(row['annotation']), \n",
        "                   row['emotion']\n",
        "                   ])\n",
        "      \n",
        "        \n",
        "\n",
        "  df = pd.DataFrame(data, columns=columns)\n",
        "  df = preprocess_dataframe(df)\n",
        "  return df\n",
        "\n",
        "# kaggle data set 을 읽기 위한 함수\n",
        "def read_csv_make_dataframe():\n",
        "  df = pd.read_csv('%s/en_data.csv' % path, encoding='latin1')\n",
        "  df = preprocess_dataframe(df)\n",
        "  df = df[['i_dialog', 'i_utterance', 'speaker', 'utterance', 'context']]\n",
        "  return df\n",
        "\n",
        "\n",
        "train = read_json_make_dataframe('train')\n",
        "print(train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "mean length of sentence: 63.43404980588959\n",
            "max length of sentence: 303\n",
            "std dev length of sentence: 49.02061864789282\n",
            "       i_dialog  ...    l\n",
            "0             0  ...    1\n",
            "1             0  ...    7\n",
            "2             0  ...   13\n",
            "3             0  ...   22\n",
            "4             0  ...   27\n",
            "...         ...  ...  ...\n",
            "10556       719  ...  129\n",
            "10557       719  ...  139\n",
            "10558       719  ...  146\n",
            "10559       719  ...  147\n",
            "10560       719  ...  160\n",
            "\n",
            "[10561 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM-wiwuxtSdO",
        "colab_type": "text"
      },
      "source": [
        "annotaion 변환 및 러닝 프로세스"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C5l8FHPtTI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# annotation 값을 emotion 값으로 변환함\n",
        "def annotation_to_emotion(annotation):\n",
        "    # 반올림 기반 변환법\n",
        "\n",
        "    # 5 곱해서 반올림 친 벡터를 만듦\n",
        "    rounded = [round(a * 5) for a in annotation]    \n",
        "\n",
        "    # non-neutral check\n",
        "    # 최대치가 같은게 두개 이상 있으면 neutral\n",
        "    max_value = max(rounded)\n",
        "    if len([x for x in rounded if x == max_value]) >= 2:\n",
        "      return \"non-neutral\"\n",
        "    \n",
        "    # 맥스 밸류 인덱스에 해당하는 이모션 리턴\n",
        "    return ['neutral','joy', 'sadness', 'fear', 'anger', 'surprise', 'disgust'][rounded.index(max_value)]\n",
        "\n",
        "def summary_y_hats(y_hats):\n",
        "  # y_hats의 분포를 보여주는 함수\n",
        "  count_dict = {}\n",
        "  \n",
        "  for i in range(0, len(y_hats)):\n",
        "    y_hat = y_hats[i]\n",
        "    prediected_emotion = annotation_to_emotion(y_hat) # 예측한 이모션 값\n",
        "    count_dict[prediected_emotion] = count_dict.get(prediected_emotion, 0) + 1\n",
        "\n",
        "  print(\"summary of y:\", count_dict)\n",
        "  return\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSO2_ioftwJ-",
        "colab_type": "text"
      },
      "source": [
        "러닝 프로세스"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67F7Wxuqtxay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 예측\n",
        "def predict(data):\n",
        "  from sklearn.metrics import f1_score, accuracy_score\n",
        "  count = len(data)\n",
        "  X = generate_X(data)\n",
        "  y = generate_Y(data) # 정답\n",
        "  y_hats = model.predict(X) # 예측치(annotation 벡터)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"summary of y\")\n",
        "  summary_y_hats(y)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"summary of y_hats\")\n",
        "  summary_y_hats(y_hats)\n",
        "\n",
        "  y_true = [x for x in data['emotion'].values]\n",
        "  y_pred = [annotation_to_emotion(x) for x in y_hats]\n",
        "\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "  print(\"accuray: \", acc)\n",
        "  print(\"f1_score: \",f1_score(y_true, y_pred, average='macro'))\n",
        "  return acc\n",
        "\n",
        "def run(index=\"\"):\n",
        "  data = read_csv_make_dataframe()\n",
        "\n",
        "  count = len(data)\n",
        "  X = generate_X(data)\n",
        "  y_hats = model.predict(X) # 예측치(annotation 벡터)\n",
        "\n",
        "  results = []\n",
        "  columns = ['Id', 'Predicted']\n",
        "  for i in range(0, count):\n",
        "    y_hat = y_hats[i]\n",
        "    prediected_emotion = annotation_to_emotion(y_hat) # 예측한 이모션 값\n",
        "\n",
        "    results.append((i, prediected_emotion))\n",
        "\n",
        "  pd.DataFrame(results, columns=columns).to_csv('%s/nlp_en_Dual_Encoder_LSTM_%s.csv' % (path, index), index=False)\n",
        "  return\n",
        "\n",
        "def learn():\n",
        "  # 테스트 데이터 읽음\n",
        "  test = read_json_make_dataframe('test')\n",
        "  acc = predict(test)\n",
        "  accuracies = []\n",
        "  for i in range(0, 3):\n",
        "    epochs = 3\n",
        "    batch_size = 256\n",
        "\n",
        "    history = model.fit(X_train, \n",
        "                        y_train, \n",
        "                        epochs=epochs, \n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True,\n",
        "    )\n",
        "    acc = predict(test)\n",
        "    accuracies.append(acc)\n",
        "    learned = epochs * (i+1)\n",
        "    run(learned)\n",
        "\n",
        "  print(\"seq of accuracies:\", accuracies)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp9mfXw4aY7C",
        "colab_type": "text"
      },
      "source": [
        "읽은 인풋으로 러닝 데이터를 생성한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4f6Vcubegi2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "96f164cc-2965-4dff-ff97-2d2181ea0668"
      },
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 50000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 350\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train['utterance'].values)\n",
        "tokenizer.fit_on_texts(train['context'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = max([len(seq) for seq in train['utterance'].values + train['context'].values])\n",
        "MAX_NB_WORDS = len(tokenizer.word_index) + 1\n",
        "\n",
        "# X 데이터 생성\n",
        "def generate_X(df):\n",
        "  X_context = tokenizer.texts_to_sequences(df['context'].values)\n",
        "  X_context = pad_sequences(X_context, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "  X_utterance = tokenizer.texts_to_sequences(df['utterance'].values)\n",
        "  X_utterance = pad_sequences(X_utterance, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  return [X_context, X_utterance]\n",
        "\n",
        "\n",
        "# Y 데이터를 만들어줌\n",
        "def generate_Y(df):\n",
        "  annotations = df['annotation'].apply(lambda x: pd.Series([float(c)/5 for c in x], dtype='float32'))\n",
        "  # 어노테이션 값을 벡터화함.\n",
        "  # 2120000 ->벡터화-> [2, 1, 2, 0, 0, 0, 0] ->나누기 5-> [0.4, 0.2, 0.4, 0, 0, 0, 0]\n",
        "  return annotations.values\n",
        "\n",
        "X_train = generate_X(train)\n",
        "y_train = generate_Y(train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6077 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmXGxA_hbF4_",
        "colab_type": "text"
      },
      "source": [
        "모델\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwEWXpRJlwr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Multiply, Input\n",
        "from keras.models import Model\n",
        "\n",
        "def create_model():\n",
        "\n",
        "  LSTM_DIM = 256\n",
        "\n",
        "  input_layer = [Input(shape=(MAX_SEQUENCE_LENGTH,)), Input(shape=(MAX_SEQUENCE_LENGTH,))]\n",
        "\n",
        "  # define lstm for context\n",
        "  embedding_context = Embedding(output_dim=EMBEDDING_DIM,\n",
        "                        input_dim=MAX_NB_WORDS,\n",
        "                        input_length=MAX_SEQUENCE_LENGTH)(input_layer[0])\n",
        "\n",
        "  lstm_context = LSTM(units=LSTM_DIM)(embedding_context)\n",
        "\n",
        "\n",
        "  embedding_utteracne = Embedding(output_dim=EMBEDDING_DIM,\n",
        "                        input_dim=MAX_NB_WORDS,\n",
        "                        input_length=MAX_SEQUENCE_LENGTH,)(input_layer[1])\n",
        "\n",
        "  lstm_utteracne = LSTM(units=LSTM_DIM)(embedding_utteracne)\n",
        "\n",
        "  multiply = Multiply()([lstm_context, lstm_utteracne])\n",
        "  dense = Dense(7, activation='softmax')(multiply)\n",
        "\n",
        "  model = Model(inputs=input_layer, outputs=dense)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = create_model()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc0O0IgqbTEH",
        "colab_type": "text"
      },
      "source": [
        "실행\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CWzWBcQ562_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2b06b7a5-8504-4c4f-ca14-fd7e1a356f28"
      },
      "source": [
        "learn()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean length of sentence: 60.91280752532562\n",
            "max length of sentence: 286\n",
            "std dev length of sentence: 47.66618446797404\n",
            "\n",
            "summary of y\n",
            "summary of y: {'surprise': 327, 'neutral': 1369, 'joy': 320, 'non-neutral': 321, 'anger': 190, 'sadness': 105, 'fear': 47, 'disgust': 85}\n",
            "\n",
            "summary of y_hats\n",
            "summary of y: {'non-neutral': 2764}\n",
            "accuray:  0.19573082489146165\n",
            "f1_score:  0.040922844175491684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "10561/10561 [==============================] - 324s 31ms/step - loss: 1.6832 - accuracy: 0.5376\n",
            "Epoch 2/3\n",
            "10561/10561 [==============================] - 321s 30ms/step - loss: 1.5793 - accuracy: 0.5519\n",
            "Epoch 3/3\n",
            "10561/10561 [==============================] - 325s 31ms/step - loss: 1.4938 - accuracy: 0.5673\n",
            "\n",
            "summary of y\n",
            "summary of y: {'surprise': 327, 'neutral': 1369, 'joy': 320, 'non-neutral': 321, 'anger': 190, 'sadness': 105, 'fear': 47, 'disgust': 85}\n",
            "\n",
            "summary of y_hats\n",
            "summary of y: {'neutral': 2174, 'non-neutral': 484, 'joy': 18, 'surprise': 87, 'sadness': 1}\n",
            "accuray:  0.46237337192474676\n",
            "f1_score:  0.15272072200507072\n",
            "mean length of sentence: 58.89381067961165\n",
            "max length of sentence: 215\n",
            "std dev length of sentence: 45.748533592551894\n",
            "Epoch 1/3\n",
            "10561/10561 [==============================] - 325s 31ms/step - loss: 1.3946 - accuracy: 0.6074\n",
            "Epoch 2/3\n",
            "10561/10561 [==============================] - 322s 30ms/step - loss: 1.3238 - accuracy: 0.6460\n",
            "Epoch 3/3\n",
            "10561/10561 [==============================] - 326s 31ms/step - loss: 1.2651 - accuracy: 0.6666\n",
            "\n",
            "summary of y\n",
            "summary of y: {'surprise': 327, 'neutral': 1369, 'joy': 320, 'non-neutral': 321, 'anger': 190, 'sadness': 105, 'fear': 47, 'disgust': 85}\n",
            "\n",
            "summary of y_hats\n",
            "summary of y: {'non-neutral': 433, 'neutral': 1918, 'sadness': 42, 'anger': 37, 'joy': 166, 'surprise': 168}\n",
            "accuray:  0.47901591895803186\n",
            "f1_score:  0.23706381063997334\n",
            "mean length of sentence: 58.89381067961165\n",
            "max length of sentence: 215\n",
            "std dev length of sentence: 45.748533592551894\n",
            "Epoch 1/3\n",
            "10561/10561 [==============================] - 332s 31ms/step - loss: 1.2072 - accuracy: 0.6928\n",
            "Epoch 2/3\n",
            "10561/10561 [==============================] - 333s 31ms/step - loss: 1.1722 - accuracy: 0.7083\n",
            "Epoch 3/3\n",
            "10561/10561 [==============================] - 322s 30ms/step - loss: 1.1306 - accuracy: 0.7269\n",
            "\n",
            "summary of y\n",
            "summary of y: {'surprise': 327, 'neutral': 1369, 'joy': 320, 'non-neutral': 321, 'anger': 190, 'sadness': 105, 'fear': 47, 'disgust': 85}\n",
            "\n",
            "summary of y_hats\n",
            "summary of y: {'surprise': 381, 'neutral': 1481, 'non-neutral': 493, 'sadness': 50, 'anger': 154, 'joy': 195, 'disgust': 9, 'fear': 1}\n",
            "accuray:  0.4392185238784371\n",
            "f1_score:  0.2568750202385146\n",
            "mean length of sentence: 58.89381067961165\n",
            "max length of sentence: 215\n",
            "std dev length of sentence: 45.748533592551894\n",
            "seq of accuracies: [0.46237337192474676, 0.47901591895803186, 0.4392185238784371]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}